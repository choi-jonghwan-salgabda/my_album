# -*- coding: utf-8 -*-
"""
1. í˜„ì¬ êµ¬ìƒí•˜ê³  ìˆëŠ”ê²ƒì€ ë‚´desktop ì»´í“¨í„°ì˜ ì‚¬ì§„ë“¤ì„ ì´ë¯¸ì§€ë§Œ í•™ìŠµí•˜ì—¬ ë¶„ë¥˜ë¥¼ í•˜ê³ ì í•©ë‹ˆë‹¤. 
2. ë‚´ê°€ ë³´ê´€ì¤‘ì¸ ì‚¬ì§„ì€ ì´ë¦„(label)ì´ ì—†ìŠµë‹ˆë‹¤.
3. ë¶„ë¥˜í›„ íŠ¹ì •ì¸ì˜ ì–¼êµ´ì„ ë…¸íŠ¸ë¶ ì¹´ë©”ë¼ë¡œ ë¹„ì¶”ê³  ì‚¬ì§„ì˜ ì´ë¦„ì„ ì…ë ¥í•˜ë©´ 
4. í•™ìŠµí•œ ì‚¬ì§„ë“¤ì¤‘ ê°€ì¥ ì¹´ë©”ë¼ë¡œ ë³´ì—¬ì¤€ ì‚¬ì§„ê³¼ ê·¼ì ‘í•œ ë¶„ë¥˜ì— ê·¸ì´ë¦„ì„ ì •ì˜í•˜ê³  
5. ê·¸ì‚¬ì§„ì´ í¬í•¨ëœ ë‹¤ë¥¸ ì‚¬ì§„ë“¤ì„ ë³´ì—¬ì£¼ë„ë¡ í• ê²ƒì…ë‹ˆë‹¤.
6. ìƒˆë¡œ ì…ë ¥ëœ ì‚¬ì§„ì´ ë¶„ë¥˜ì— ì†í•˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ë¶„ë¥˜ë¥¼ ì¶”ê°€í•  ê²ƒì…ë‹ˆë‹¤. ì´ë•Œì—ëŠ” ì´ë¦„ì´ ìˆì„ìˆ˜ ìˆê³  ì—†ì„ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
7. ìƒˆë¡œìš´ ë¶„ë¥˜ ë˜ëŠ” ì´ë¯¸ì§€ê°€ í•™ìŠµë˜ë©´ ì „ì²´ì˜ indexë¥¼ ì¬ ì„¤ì • í•  ê²ƒì…ë‹ˆë‹¤.
8. í–¥í›„ ì´ê²ƒì„ ì„¤êµ´ì‚¬ì§„ì™¸ì˜ ìì—°ì‚¬ì§„, ê°„íŒì‚¬ì§„ë“±ì— í™•ì¥í•  ê²ƒì…ë‹ˆë‹¤. 

ë¨¼ì € train_myresnet.pyë¥¼ ìˆ˜ì •í•˜ì—¬ ë””ë ‰í† ë¦¬ ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ ê¸°ëŠ¥ì„ ì•ˆì •ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
ì–¼êµ´ ê²€ì¶œ ë° ì„ë² ë”© ì¶”ì¶œ ê¸°ëŠ¥ì„ ë³„ë„ë¡œ êµ¬í˜„í•˜ê³  í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
ì¶”ì¶œëœ ì–¼êµ´ ì„ë² ë”©ê³¼ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ê°„ë‹¨í•œ ë°©ë²•ì„ êµ¬í˜„í•©ë‹ˆë‹¤ (ì˜ˆ: ë”•ì…”ë„ˆë¦¬, JSON íŒŒì¼, SQLite).
ìœ ì‚¬ë„ ê²€ìƒ‰ ë° ì´ë¦„ íƒœê¹… ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ë° ì ì§„ì  í•™ìŠµ/ì¸ë±ì‹± ë°©ë²•ì„ ì—°êµ¬í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.


Gemini
ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤! ë ˆì´ë¸” ì—†ëŠ” ì´ë¯¸ì§€ë“¤ë¡œ ë¹„ì§€ë„ í•™ìŠµ ë˜ëŠ” ìê¸°ì§€ë„ í•™ìŠµì„ í†µí•´ ëª¨ë¸ì´ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ë°°ìš°ê²Œ í•œ ë‹¤ìŒ, ì‚¬ìš©ìë‹˜ì˜ ì–¼êµ´ ì‚¬ì§„ì„ ì…ë ¥í•˜ë©´ ê·¸ ëª¨ë¸ì´ í•™ìŠµí–ˆë˜ ë°ì´í„° ì¤‘ì—ì„œ ì‚¬ìš©ìë‹˜ì˜ ì–¼êµ´ì´ í¬í•¨ëœ ë‹¤ë¥¸ ì‚¬ì§„ë“¤ì„ ì°¾ì•„ ì¶œë ¥í•˜ëŠ” ì‹œìŠ¤í…œì„ ë§Œë“¤ê³  ì‹¶ìœ¼ì‹  ê±°êµ°ìš”.


ì´ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì£¼ìš” ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

íŠ¹ì§• ì¶”ì¶œ ëª¨ë¸ ì¤€ë¹„:

ì˜µì…˜ A (ì‚¬ì „ í•™ìŠµ ëª¨ë¸ í™œìš©): ImageNet ë“±ìœ¼ë¡œ ë¯¸ë¦¬ í•™ìŠµëœ ëª¨ë¸(ì˜ˆ: timmì˜ ResNet, Vision Transformer ë“±)ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì´ë¯¸ ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì˜ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–¼êµ´ ì¸ì‹ ì„±ëŠ¥ì„ ë†’ì´ë ¤ë©´ ì–¼êµ´ ì¸ì‹ìš©ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(ì˜ˆ: FaceNet, ArcFace ê¸°ë°˜ ëª¨ë¸)ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ìŠµë‹ˆë‹¤. (face_recognition ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì´ëŸ° ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.)
ì˜µì…˜ B (ìê¸°ì§€ë„ í•™ìŠµ): ê°€ì§€ê³  ê³„ì‹  ë ˆì´ë¸” ì—†ëŠ” ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ SimCLR, MoCo ê°™ì€ ìê¸°ì§€ë„ í•™ìŠµ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ íŠ¹ì§• ì¶”ì¶œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. (ì´ ë°©ë²•ì€ ìƒë‹¹í•œ ì»´í“¨íŒ… ìì›ê³¼ ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)
ë°ì´í„°ì…‹ íŠ¹ì§• ì¶”ì¶œ ë° ì¸ë±ì‹±:

ì¤€ë¹„ëœ íŠ¹ì§• ì¶”ì¶œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì§€ê³  ê³„ì‹  ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•´ íŠ¹ì§• ë²¡í„°(ì„ë² ë”©)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
ì–¼êµ´ ì¸ì‹ ê°•í™”: ë” ì •í™•í•˜ê²Œ ì‚¬ìš©ìë‹˜ì˜ ì–¼êµ´ì„ ì°¾ìœ¼ë ¤ë©´, ê° ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ì˜ì—­ì„ ë¨¼ì € ê²€ì¶œí•˜ê³  (ì˜ˆ: face_recognition ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©), ê²€ì¶œëœ ì–¼êµ´ ì˜ì—­ì— ëŒ€í•´ì„œë§Œ íŠ¹ì§• ë²¡í„°(ì–¼êµ´ ì„ë² ë”©)ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
ì¶”ì¶œëœ ëª¨ë“  (ì–¼êµ´) ì„ë² ë”©ê³¼ í•´ë‹¹ ì´ë¯¸ì§€ê°€ ì–´ë–¤ íŒŒì¼ì¸ì§€(íŒŒì¼ ê²½ë¡œ)ë¥¼ í•¨ê»˜ ì €ì¥í•˜ê³ , ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•´ ì¸ë±ì‹±í•©ë‹ˆë‹¤. (ê°„ë‹¨í•˜ê²ŒëŠ” NumPy ë°°ì—´ê³¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬í•˜ê±°ë‚˜, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì˜ ê²½ìš° Faiss ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.)
ì¿¼ë¦¬ ë° ê²€ìƒ‰:

ì‚¬ìš©ìë‹˜ì´ ë³¸ì¸ ì–¼êµ´ ì‚¬ì§„(ì¿¼ë¦¬ ì´ë¯¸ì§€)ì„ ì…ë ¥í•©ë‹ˆë‹¤.
ì¿¼ë¦¬ ì´ë¯¸ì§€ì—ì„œë„ ì–¼êµ´ ì˜ì—­ì„ ê²€ì¶œí•˜ê³ , ë™ì¼í•œ íŠ¹ì§• ì¶”ì¶œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ (ì–¼êµ´) ì„ë² ë”©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
ì´ ì¿¼ë¦¬ ì„ë² ë”©ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì„ë² ë”©ë“¤ì„ ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ì˜ˆ: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë˜ëŠ” ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì‚¬ìš©)
ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œ, ë‹¨ìˆœíˆ ê°€ì¥ ê°€ê¹Œìš´ ëª‡ ê°œë¥¼ ì°¾ëŠ” ê²ƒë³´ë‹¤ ì¼ì • ìœ ì‚¬ë„ ì„ê³„ê°’(threshold) ì´ìƒì¸ ì„ë² ë”©ë§Œ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ë¥¸ ì‚¬ëŒì˜ ì–¼êµ´ì´ë‚˜ ì–¼êµ´ì´ ì•„ë‹Œ ì´ë¯¸ì§€ê°€ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ëŠ” ê²ƒì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ê²°ê³¼ ì¶œë ¥:

ê²€ìƒ‰ëœ ìœ ì‚¬ ì„ë² ë”©ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë“¤ì„ ê°€ì ¸ì™€ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
êµ¬í˜„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ˆì‹œ:

"""

import os
import sys
import yaml
import random
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from PIL import Image, ExifTags
import albumentations as A
from albumentations.pytorch import ToTensorV2
from tqdm import tqdm
import timm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split # ë°ì´í„° ë¶„í• ìš©

# --- ê²½ë¡œ ì„¤ì • (ê°œì„ ëœ ë°©ì‹) ---
try:
    # 1. í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ì–»ê¸°
    current_file_path = Path(__file__).resolve()
    # 2. ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ (sorc ë””ë ‰í† ë¦¬)
    WORK_DIR = current_file_path.parent
    # 3. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ (WORK_DIRì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬)
    PROJ_DIR = WORK_DIR.parent

    # 4. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆëŠ” .my_config.yaml ë¡œë“œ
    dir_config_path_yaml = PROJ_DIR / ".my_config.yaml" # pathlibì˜ / ì—°ì‚°ì ì‚¬ìš©
    if not dir_config_path_yaml.is_file():
        print(f"âŒ í”„ë¡œì íŠ¸ ê²½ë¡œêµ¬ì„± ì„¤ì • íŒŒì¼({dir_config_path_yaml})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit("í”„ë¡œì íŠ¸ ê²½ë¡œêµ¬ì„± ì„¤ì • íŒŒì¼ ì—†ìŒ")

    with open(dir_config_path_yaml, "r", encoding="utf-8") as file:
        dir_config = yaml.safe_load(file)

    # --- í•„ìš”í•œ ê²½ë¡œ ë³€ìˆ˜ ì„¤ì • (Path ê°ì²´ ì‚¬ìš© ë° get í™œìš©) ---
    # ì„¤ì • íŒŒì¼ì—ì„œ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ í™•ì¸ (ì„ íƒì )
    config_proj_dir = Path(dir_config.get('resnet34_path', {}).get('PROJ_DIR', str(PROJ_DIR)))
    if PROJ_DIR != config_proj_dir:
        print(f"ê²½ê³ : ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ë°˜ í”„ë¡œì íŠ¸ ê²½ë¡œ({PROJ_DIR})ì™€ ì„¤ì • íŒŒì¼ ê²½ë¡œ({config_proj_dir})ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")
        # í•„ìš”ì‹œ ì—¬ê¸°ì„œ ì²˜ë¦¬ ë˜ëŠ” ì¢…ë£Œ
        # PROJ_DIR = config_proj_dir # ì„¤ì • íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ë§ì¶œ ê²½ìš°

    # ë‚˜ë¨¸ì§€ ê²½ë¡œë“¤ì„ PROJ_DIR ê¸°ì¤€ìœ¼ë¡œ ì„¤ì • (ì„¤ì • íŒŒì¼ ê°’ ìš°ì„ )
    resnet34_paths = dir_config.get('resnet34_path', {})
    worker_paths = resnet34_paths.get('worker_path', {})
    dataset_paths = resnet34_paths.get('dataset_path', {})
    output_paths = resnet34_paths.get('output_path', {})

    # ê¸°ë³¸ê°’ ì„¤ì • ì‹œ PROJ_DIR ê¸°ë°˜ìœ¼ë¡œ ì„¤ì •
    WORK_DIR = Path(worker_paths.get('WORK_DIR', PROJ_DIR / 'sorc')) # WORK_DIR ì¬ì •ì˜ (ì„¤ì • ìš°ì„ )
    LOG_DIR = Path(worker_paths.get('LOG_DIR', WORK_DIR / 'logs'))
    MODEL_CFG_DIR = Path(worker_paths.get('CONFIG_DIR', WORK_DIR / 'config')) # ëª¨ë¸ ì„¤ì • ë””ë ‰í† ë¦¬ (ë³€ìˆ˜ëª… ë³€ê²½: MODEL_DIR -> MODEL_CFG_DIR)
    DATA_DIR = Path(dataset_paths.get('DATA_DIR', PROJ_DIR / 'data'))
    IMAGES_DIR = Path(dataset_paths.get('IMAGES_DIR', DATA_DIR / 'images'))
    JSONS_DIR = Path(dataset_paths.get('JSONS_DIR', DATA_DIR / 'jsons')) # JSONS_DIR ì¶”ê°€ (í•„ìš”ì‹œ)
    OUTPUT_DIR = Path(output_paths.get('OUTPUT_DIR', PROJ_DIR / 'outputs'))

    # ì¶œë ¥ ë° ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)

    # ëª¨ë¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
    model_config_path = MODEL_CFG_DIR / ".model.yaml" # MODEL_CFG_DIR ì‚¬ìš©

except FileNotFoundError as e:
    print(f"âŒ ì„¤ì • íŒŒì¼ ê´€ë ¨ ì˜¤ë¥˜: {e}")
    sys.exit(1)
except KeyError as e:
    print(f"âŒ ì„¤ì • íŒŒì¼ í‚¤ ì˜¤ë¥˜: í•„ìš”í•œ í‚¤ '{e}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)
except Exception as e:
    print(f"âŒ ì´ˆê¸° ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc() # ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥
    sys.exit(1)

# --- ë¡œê±° ì„¤ì • ---
worker_name = Path(__file__).stem # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (í™•ì¥ì ì œì™¸)
log_file_path = LOG_DIR / f"{worker_name}.log" # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ

# ê¸°ë³¸ ë¡œê±° ì„¤ì • (íŒŒì¼ ë° ì½˜ì†” ì¶œë ¥)
logging.basicConfig(level=logging.INFO, # ë¡œê·¸ ë ˆë²¨ INFOë¡œ ì„¤ì • (DEBUGëŠ” ë„ˆë¬´ ìƒì„¸í•  ìˆ˜ ìˆìŒ)
                    format='%(asctime)s - %(levelname)-6s - %(filename)s:%(lineno)d - %(message)s',
                    handlers=[logging.StreamHandler(sys.stdout), # ì½˜ì†” ì¶œë ¥ í•¸ë“¤ëŸ¬
                              logging.FileHandler(log_file_path, encoding='utf-8')]) # íŒŒì¼ ì¶œë ¥ í•¸ë“¤ëŸ¬
logger = logging.getLogger(__name__) # ë¡œê±° ê°ì²´ ìƒì„±

logger.info(f"--- ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘: {worker_name} ---")
logger.info(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸ (PROJ_DIR): {PROJ_DIR}")
logger.info(f"ì‘ì—… ë””ë ‰í† ë¦¬ (WORK_DIR) : {WORK_DIR}")
logger.info(f"ë°ì´í„° ë£¨íŠ¸ (DATA_DIR)  : {DATA_DIR}")
logger.info(f"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ (IMAGES_DIR): {IMAGES_DIR}")
logger.info(f"JSON ë””ë ‰í† ë¦¬ (JSONS_DIR) : {JSONS_DIR}") # JSONS_DIR ë¡œê¹… ì¶”ê°€
logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ (OUTPUT_DIR): {OUTPUT_DIR}")
logger.info(f"ë¡œê·¸ ë””ë ‰í† ë¦¬ (LOG_DIR)   : {LOG_DIR}")
logger.info(f"ëª¨ë¸ ì„¤ì • ë””ë ‰í† ë¦¬ (MODEL_CFG_DIR): {MODEL_CFG_DIR}")
logger.info(f"ëª¨ë¸ ì„¤ì • íŒŒì¼: {model_config_path}")

# --- ëª¨ë¸ ì„¤ì • ë¡œë“œ ---
if not model_config_path.is_file():
    logger.error(f"âŒ ëª¨ë¸ ì„¤ì • íŒŒì¼({model_config_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit("ëª¨ë¸ ì„¤ì • íŒŒì¼ ì—†ìŒ")

try:
    with open(model_config_path, "r", encoding="utf-8") as file:
        model_config = yaml.safe_load(file)
        # ì„¤ì •ê°’ ë¡œë“œ ë° ê¸°ë³¸ê°’ ì„¤ì • (get, setdefault ì‚¬ìš©)
        model_config['LR'] = float(model_config.get('LR', 1e-3))
        model_config.setdefault('EPOCHS', 10)
        model_config.setdefault('BATCH_SIZE', 32)
        model_config.setdefault('NUM_WORKERS', 2)
        model_config.setdefault('img_size', 224) # ì´ë¯¸ì§€ í¬ê¸° ê¸°ë³¸ê°’ ì„¤ì •
        model_config.setdefault('model_name', 'resnet34')
        model_config.setdefault('SEED', 42)
        logger.info("ëª¨ë¸ ì„¤ì • ë¡œë“œ ì™„ë£Œ.")
        logger.debug(f"ëª¨ë¸ ì„¤ì • ë‚´ìš©: {model_config}") # DEBUG ë ˆë²¨ë¡œ ì„¤ì • ë‚´ìš© ë¡œê¹…
except FileNotFoundError as e:
    logger.error(f"âŒ ëª¨ë¸ ì„¤ì • íŒŒì¼ ê´€ë ¨ ì˜¤ë¥˜: {e}")
    sys.exit(1)
except Exception as e:
    logger.error(f"âŒ ëª¨ë¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True) # ìƒì„¸ ì˜¤ë¥˜ ë¡œê¹…
    sys.exit(1)

# --- ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” í•¨ìˆ˜ ì œê±° ---
# def substitute_vars(data): # ì´ í•¨ìˆ˜ëŠ” ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤.
#     ...

# --- í™˜ê²½ ì´ˆê¸°í™” ---
def init_env(seed: int):
    """í™˜ê²½ ì‹œë“œ ê³ ì • í•¨ìˆ˜"""
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if using multi-GPU
    # torch.backends.cudnn.deterministic = True # ì£¼ì„ ì²˜ë¦¬ (ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„±)
    torch.backends.cudnn.benchmark = True # ì¼ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ (ì…ë ¥ í¬ê¸° ê³ ì • ì‹œ)
    logger.info(f"í™˜ê²½ ì‹œë“œ ê³ ì • ì™„ë£Œ (SEED={seed})")

# --- ì´ë¯¸ì§€ íšŒì „ ì²˜ë¦¬ ---
# EXIF Orientation íƒœê·¸ ê°’ ì°¾ê¸°
EXIF_ORIENTATION_TAG = None
for k, v in ExifTags.TAGS.items():
    if v == 'Orientation':
        EXIF_ORIENTATION_TAG = k
        break

def rotate_image_based_on_exif(image: Image.Image) -> Image.Image:
    """EXIF ì •ë³´ì— ë”°ë¼ ì´ë¯¸ì§€ë¥¼ íšŒì „ì‹œí‚µë‹ˆë‹¤."""
    if EXIF_ORIENTATION_TAG is None: return image # Orientation íƒœê·¸ ì •ë³´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
    try:
        exif = image.getexif() # ì´ë¯¸ì§€ì—ì„œ EXIF ë°ì´í„° ì¶”ì¶œ
        orientation = exif.get(EXIF_ORIENTATION_TAG) # Orientation ê°’ ê°€ì ¸ì˜¤ê¸°
    except Exception: # EXIF ì •ë³´ê°€ ì—†ê±°ë‚˜ ì†ìƒëœ ê²½ìš°
        orientation = None

    # Orientation ê°’ì— ë”°ë¼ ì´ë¯¸ì§€ íšŒì „ ë˜ëŠ” ë°˜ì „ ì ìš©
    if orientation == 2: return image.transpose(Image.FLIP_LEFT_RIGHT) # ì¢Œìš° ë°˜ì „
    elif orientation == 3: return image.rotate(180) # 180ë„ íšŒì „
    elif orientation == 4: return image.rotate(180).transpose(Image.FLIP_LEFT_RIGHT) # 180ë„ íšŒì „ í›„ ì¢Œìš° ë°˜ì „
    elif orientation == 5: return image.rotate(-90, expand=True).transpose(Image.FLIP_LEFT_RIGHT) # ì‹œê³„ë°©í–¥ 90ë„ íšŒì „ í›„ ì¢Œìš° ë°˜ì „
    elif orientation == 6: return image.rotate(-90, expand=True) # ì‹œê³„ë°©í–¥ 90ë„ íšŒì „
    elif orientation == 7: return image.rotate(90, expand=True).transpose(Image.FLIP_LEFT_RIGHT) # ë°˜ì‹œê³„ë°©í–¥ 90ë„ íšŒì „ í›„ ì¢Œìš° ë°˜ì „
    elif orientation == 8: return image.rotate(90, expand=True) # ë°˜ì‹œê³„ë°©í–¥ 90ë„ íšŒì „
    return image # í•´ë‹¹ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜

# --- ì´ë¯¸ì§€ ë¶„ë¥˜ ë°ì´í„°ì…‹ ---
class ClassificationDataset(Dataset):
    """
    train.csv íŒŒì¼ì„ ì½ì–´ ì´ë¯¸ì§€ ë¶„ë¥˜ í•™ìŠµìš© ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” Dataset í´ë˜ìŠ¤.
    """
    def __init__(self, df: pd.DataFrame, image_root: Path, transform: Optional[A.Compose] = None):
        """
        Args:
            df (pd.DataFrame): 'ID' (ì´ë¯¸ì§€ íŒŒì¼ëª…)ì™€ 'target' (ë ˆì´ë¸”) ì»¬ëŸ¼ì„ í¬í•¨í•˜ëŠ” ë°ì´í„°í”„ë ˆì„.
            image_root (Path): ì´ë¯¸ì§€ íŒŒì¼ë“¤ì´ ìˆëŠ” ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
            transform (Optional[A.Compose]): Albumentations ë³€í™˜ íŒŒì´í”„ë¼ì¸.
        """
        self.df = df.reset_index(drop=True) # ì¸ë±ìŠ¤ ì¬ì„¤ì • (iloc ì ‘ê·¼ ìš©ì´)
        self.image_root = image_root
        self.transform = transform
        logger.info(f"Dataset ìƒì„±: {len(self.df)}ê°œ ìƒ˜í”Œ, ì´ë¯¸ì§€ ë£¨íŠ¸: {self.image_root}")

    def __len__(self) -> int:
        """ë°ì´í„°ì…‹ì˜ ì´ ìƒ˜í”Œ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return len(self.df)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        """ë°ì´í„°ì…‹ì—ì„œ ì¸ë±ìŠ¤(idx)ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì¼ ìƒ˜í”Œ(ì´ë¯¸ì§€ í…ì„œ, ë ˆì´ë¸”)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        image_id = self.df.loc[idx, 'ID'] # ì´ë¯¸ì§€ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°
        target = int(self.df.loc[idx, 'target']) # ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸° (ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜)
        image_path = self.image_root / image_id # ì´ë¯¸ì§€ ì „ì²´ ê²½ë¡œ ìƒì„±

        try:
            # PILì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¡œë“œ ë° RGB ë³€í™˜
            image = Image.open(image_path).convert('RGB')
            # EXIF ì •ë³´ ê¸°ë°˜ íšŒì „ ì ìš©
            image = rotate_image_based_on_exif(image)
            # Albumentations ì ìš©ì„ ìœ„í•´ NumPy ë°°ì—´ë¡œ ë³€í™˜
            image_np = np.array(image)

        except FileNotFoundError:
            logger.warning(f"ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_path} (ì¸ë±ìŠ¤ {idx}). ë”ë¯¸ ë°ì´í„° ë°˜í™˜.")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜ (ëª¨ë¸ ì…ë ¥ í¬ê¸°ì— ë§ì¶°ì„œ)
            dummy_image = torch.zeros((3, model_config['img_size'], model_config['img_size']), dtype=torch.float32)
            return dummy_image, -1 # ì˜¤ë¥˜ ì‹ë³„ìš© ë ˆì´ë¸” (-1)
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ë¡œë”©/ì²˜ë¦¬ ì˜¤ë¥˜ {image_path}: {e}. ë”ë¯¸ ë°ì´í„° ë°˜í™˜.", exc_info=True)
            dummy_image = torch.zeros((3, model_config['img_size'], model_config['img_size']), dtype=torch.float32)
            return dummy_image, -1

        # Albumentations ë³€í™˜ ì ìš©
        if self.transform:
            try:
                transformed = self.transform(image=image_np) # ë³€í™˜ ì ìš©
                image_tensor = transformed['image'] # ë³€í™˜ëœ ì´ë¯¸ì§€ í…ì„œ ì¶”ì¶œ
            except Exception as e:
                 logger.error(f"Albumentations ë³€í™˜ ì ìš© ì˜¤ë¥˜ {image_path}: {e}. ë”ë¯¸ ë°ì´í„° ë°˜í™˜.", exc_info=True)
                 image_tensor = torch.zeros((3, model_config['img_size'], model_config['img_size']), dtype=torch.float32)
                 return image_tensor, -1
        else:
            # ë³€í™˜(transform)ì´ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš°, ê¸°ë³¸ì ì¸ Tensor ë³€í™˜ ìˆ˜í–‰ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
            logger.warning("Albumentations ë³€í™˜(transform)ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ Tensor ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).float() / 255.0 # HWC -> CHW, 0-1 ì •ê·œí™”

        return image_tensor, target

# --- ë°ì´í„° ë¡œë” ìƒì„± ---
def build_dataloader(df: pd.DataFrame, image_root: Path, model_cfg: Dict, is_train: bool) -> DataLoader:
    """
    ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„ê³¼ ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ PyTorch DataLoaderë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        df (pd.DataFrame): ë°ì´í„° ì •ë³´ (ID, target)ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„.
        image_root (Path): ì´ë¯¸ì§€ íŒŒì¼ ë£¨íŠ¸ ê²½ë¡œ.
        model_cfg (Dict): ëª¨ë¸ ë° í•™ìŠµ ê´€ë ¨ ì„¤ì •ê°’ ë”•ì…”ë„ˆë¦¬.
        is_train (bool): í•™ìŠµìš© ë°ì´í„° ë¡œë”ì¸ì§€ ì—¬ë¶€ (True: í•™ìŠµìš©, False: ê²€ì¦/í…ŒìŠ¤íŠ¸ìš©).

    Returns:
        DataLoader: ìƒì„±ëœ PyTorch DataLoader ê°ì²´.
    """
    img_size = model_cfg['img_size']
    batch_size = model_cfg['BATCH_SIZE']
    num_workers = model_cfg['NUM_WORKERS']

    # ë°ì´í„° ì¦ê°•(Augmentation) ë° ì „ì²˜ë¦¬ ì„¤ì •
    if is_train:
        # í•™ìŠµìš© ë³€í™˜: í¬ê¸° ì¡°ì •, ì¢Œìš° ë°˜ì „, íšŒì „, ë°ê¸°/ëŒ€ë¹„ ì¡°ì ˆ, ì •ê·œí™”, í…ì„œ ë³€í™˜
        transform = A.Compose([
            A.Resize(height=img_size, width=img_size),
            A.HorizontalFlip(p=0.5), # 50% í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „
            A.Rotate(limit=15, p=0.3), # -15 ~ +15ë„ ë²”ìœ„ ë‚´ì—ì„œ 30% í™•ë¥ ë¡œ íšŒì „
            A.RandomBrightnessContrast(p=0.2), # 20% í™•ë¥ ë¡œ ë°ê¸°/ëŒ€ë¹„ ëœë¤ ì¡°ì ˆ
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # ImageNet í‘œì¤€ ì •ê·œí™”
            ToTensorV2(), # NumPy ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë³€í™˜ (CHW ìˆœì„œ)
        ])
        logger.info("í•™ìŠµìš© ë°ì´í„° ë³€í™˜ ì„¤ì • ì™„ë£Œ (ì¦ê°• í¬í•¨).")
    else:
        # ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© ë³€í™˜: í¬ê¸° ì¡°ì •, ì •ê·œí™”, í…ì„œ ë³€í™˜ (ë°ì´í„° ì¦ê°• ì—†ìŒ)
        transform = A.Compose([
            A.Resize(height=img_size, width=img_size),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ])
        logger.info("ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë³€í™˜ ì„¤ì • ì™„ë£Œ (ì¦ê°• ì—†ìŒ).")

    # ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    dataset = ClassificationDataset(df=df, image_root=image_root, transform=transform)

    # DataLoader ìƒì„±
    loader = DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        shuffle=is_train, # í•™ìŠµ ì‹œì—ë§Œ ë°ì´í„° ì„ê¸° (is_train=True)
        num_workers=num_workers, # ë°ì´í„° ë¡œë”© ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
        pin_memory=True, # GPU ì‚¬ìš© ì‹œ ë°ì´í„° ì „ì†¡ ì†ë„ í–¥ìƒì„ ìœ„í•´ ë©”ëª¨ë¦¬ ê³ ì •
        drop_last=False # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ë°°ì¹˜ í¬ê¸°ë³´ë‹¤ ì‘ë”ë¼ë„ ì‚¬ìš©
    )
    logger.info(f"{'í•™ìŠµ' if is_train else 'ê²€ì¦'} DataLoader ìƒì„± ì™„ë£Œ (Batch size: {batch_size}, Num workers: {num_workers})")
    return loader

# --- í•™ìŠµ ë° ê²€ì¦ ë£¨í”„ ---
def train_one_epoch(loader: DataLoader, model: nn.Module, optimizer: optim.Optimizer, loss_fn: nn.Module, device: torch.device, epoch_num: int, total_epochs: int) -> Dict[str, float]:
    """1 ì—í­(epoch) í•™ìŠµì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    model.train() # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì • (Dropout, BatchNorm ë“± í™œì„±í™”)
    total_loss = 0.0
    all_preds = [] # ëª¨ë“  ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_targets = [] # ëª¨ë“  ì‹¤ì œ ë ˆì´ë¸”ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    dummy_data_count = 0 # ë”ë¯¸ ë°ì´í„° ê°œìˆ˜ ì¹´ìš´íŠ¸

    # tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë¥  í‘œì‹œ (ì—í­ ì •ë³´ í¬í•¨)
    pbar = tqdm(loader, desc=f"Epoch {epoch_num+1}/{total_epochs} Training")
    for images, targets in pbar:
        # ë°ì´í„°ë¥¼ ì§€ì •ëœ ì¥ì¹˜(GPU ë˜ëŠ” CPU)ë¡œ ì´ë™
        images = images.to(device, non_blocking=True) # non_blocking=True: ë¹„ë™ê¸° ì „ì†¡ (GPU ì‚¬ìš© ì‹œ)
        targets = targets.to(device, non_blocking=True)

        # ë°ì´í„°ì…‹ì—ì„œ ì˜¤ë¥˜ë¡œ ì¸í•´ ë°˜í™˜ëœ ë”ë¯¸ ë°ì´í„°(-1 ë ˆì´ë¸”) ê±´ë„ˆë›°ê¸°
        if -1 in targets:
            num_dummy = torch.sum(targets == -1).item()
            dummy_data_count += num_dummy
            logger.warning(f"ë”ë¯¸ ë°ì´í„° {num_dummy}ê°œ í¬í•¨ëœ ë°°ì¹˜ë¥¼ ê±´ë„ˆ<0xEB><0><0x8F>ë‹ˆë‹¤.")
            continue

        # ì˜µí‹°ë§ˆì´ì € ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        optimizer.zero_grad()

        # ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ (Forward pass)
        preds = model(images)

        # ì†ì‹¤ ê³„ì‚°
        loss = loss_fn(preds, targets)

        # ì†ì‹¤ ê°’ì´ NaNì¸ì§€ í™•ì¸ (í•™ìŠµ ë¶ˆì•ˆì •ì„± ê°ì§€)
        if torch.isnan(loss):
            logger.error(f"NaN ì†ì‹¤ ê°ì§€ (Epoch {epoch_num+1}). ë°°ì¹˜ë¥¼ ê±´ë„ˆ<0xEB><0><0x8F>ë‹ˆë‹¤.")
            continue

        # ì—­ì „íŒŒ ìˆ˜í–‰ (Backward pass)
        loss.backward()
        # ì˜µí‹°ë§ˆì´ì € íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        optimizer.step()

        # ë°°ì¹˜ ì†ì‹¤ ëˆ„ì  (ë°ì´í„°ì…‹ ì „ì²´ í‰ê·  ê³„ì‚° ìœ„í•¨)
        total_loss += loss.item() * images.size(0) # ë°°ì¹˜ í¬ê¸° ê³±í•´ì„œ ë”í•¨
        # ì˜ˆì¸¡ê°’(ê°€ì¥ í™•ë¥  ë†’ì€ í´ë˜ìŠ¤ ì¸ë±ìŠ¤)ê³¼ ì‹¤ì œ ë ˆì´ë¸” ì €ì¥ (ë‚˜ì¤‘ì— ì •í™•ë„, F1 ê³„ì‚°ìš©)
        all_preds.extend(preds.argmax(dim=1).detach().cpu().numpy())
        all_targets.extend(targets.detach().cpu().numpy())

        # ì§„í–‰ë¥  í‘œì‹œì¤„ì— í˜„ì¬ ë°°ì¹˜ ì†ì‹¤ í‘œì‹œ (ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€)
        pbar.set_postfix(loss=f"{loss.item():.4f}")

    # ì—í­ í‰ê·  ì†ì‹¤ ê³„ì‚°
    avg_loss = total_loss / (len(loader.dataset) - dummy_data_count) if (len(loader.dataset) - dummy_data_count) > 0 else 0.0
    # ì •í™•ë„ ê³„ì‚°
    accuracy = accuracy_score(all_targets, all_preds)
    # F1 ìŠ¤ì½”ì–´ ê³„ì‚° (Macro average: ê° í´ë˜ìŠ¤ F1 ì ìˆ˜ í‰ê· , zero_division=0: íŠ¹ì • í´ë˜ìŠ¤ ì˜ˆì¸¡ ì—†ì–´ë„ ì˜¤ë¥˜ ë°©ì§€)
    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)

    # ë”ë¯¸ ë°ì´í„° ì²˜ë¦¬ ë¡œê·¸
    if dummy_data_count > 0:
        logger.warning(f"Epoch {epoch_num+1} Training: ì´ {dummy_data_count}ê°œì˜ ë”ë¯¸ ë°ì´í„° ì²˜ë¦¬ë¨.")

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    return {"train_loss": avg_loss, "train_acc": accuracy, "train_f1": f1}

def validate_one_epoch(loader: DataLoader, model: nn.Module, loss_fn: nn.Module, device: torch.device, epoch_num: int, total_epochs: int) -> Dict[str, float]:
    """1 ì—í­(epoch) ê²€ì¦ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (Dropout, BatchNorm ë“± ë¹„í™œì„±í™”)
    total_loss = 0.0
    all_preds = []
    all_targets = []
    dummy_data_count = 0

    # tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë¥  í‘œì‹œ
    pbar = tqdm(loader, desc=f"Epoch {epoch_num+1}/{total_epochs} Validation")
    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ)
    with torch.no_grad():
        for images, targets in pbar:
            images = images.to(device, non_blocking=True)
            targets = targets.to(device, non_blocking=True)

            # ë”ë¯¸ ë°ì´í„° ê±´ë„ˆë›°ê¸°
            if -1 in targets:
                num_dummy = torch.sum(targets == -1).item()
                dummy_data_count += num_dummy
                continue

            # ëª¨ë¸ ì˜ˆì¸¡
            preds = model(images)
            # ì†ì‹¤ ê³„ì‚°
            loss = loss_fn(preds, targets)

            # NaN ì†ì‹¤ í™•ì¸
            if torch.isnan(loss):
                logger.error(f"NaN ì†ì‹¤ ê°ì§€ (Epoch {epoch_num+1} Validation). ë°°ì¹˜ë¥¼ ê±´ë„ˆ<0xEB><0><0x8F>ë‹ˆë‹¤.")
                continue

            # ì†ì‹¤ ëˆ„ì  ë° ì˜ˆì¸¡/ì‹¤ì œ ë ˆì´ë¸” ì €ì¥
            total_loss += loss.item() * images.size(0)
            all_preds.extend(preds.argmax(dim=1).cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            # ì§„í–‰ë¥  í‘œì‹œì¤„ì— ì†ì‹¤ í‘œì‹œ
            pbar.set_postfix(loss=f"{loss.item():.4f}")

    # í‰ê·  ì†ì‹¤, ì •í™•ë„, F1 ìŠ¤ì½”ì–´ ê³„ì‚°
    avg_loss = total_loss / (len(loader.dataset) - dummy_data_count) if (len(loader.dataset) - dummy_data_count) > 0 else 0.0
    accuracy = accuracy_score(all_targets, all_preds)
    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)

    # ë”ë¯¸ ë°ì´í„° ì²˜ë¦¬ ë¡œê·¸
    if dummy_data_count > 0:
        logger.warning(f"Epoch {epoch_num+1} Validation: ì´ {dummy_data_count}ê°œì˜ ë”ë¯¸ ë°ì´í„° ì²˜ë¦¬ë¨.")

    return {"val_loss": avg_loss, "val_acc": accuracy, "val_f1": f1}


# --- ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ ---
def training():
    """ë©”ì¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    # í™˜ê²½ ì´ˆê¸°í™” (ì‹œë“œ ê³ ì •)
    init_env(seed=model_config['SEED'])

    # ì¥ì¹˜ ì„¤ì • (GPU ìš°ì„  ì‚¬ìš©)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # --- ë°ì´í„° ë¡œë“œ ë° ë¶„í•  ---
    try:
        train_csv_path = DATA_DIR / "train.csv" # DATA_DIR ì‚¬ìš©
        if not train_csv_path.is_file():
            logger.error(f"í•™ìŠµ ë°ì´í„° íŒŒì¼({train_csv_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        # train.csv íŒŒì¼ ë¡œë“œ
        full_df = pd.read_csv(train_csv_path)
        logger.info(f"{train_csv_path} ë¡œë“œ ì™„ë£Œ. ì´ {len(full_df)}ê°œ ë°ì´í„°.")

        # í´ë˜ìŠ¤ ê°œìˆ˜ í™•ì¸ (meta.csv ë˜ëŠ” train.csvì˜ target ê°’ ê¸°ì¤€)
        meta_csv_path = DATA_DIR / "meta.csv" # DATA_DIR ì‚¬ìš©
        if meta_csv_path.is_file():
            meta_df = pd.read_csv(meta_csv_path)
            num_classes = len(meta_df)
            logger.info(f"meta.csv ê¸°ë°˜ í´ë˜ìŠ¤ ê°œìˆ˜: {num_classes}")
        else:
            num_classes = full_df['target'].nunique() # train.csvì—ì„œ ê³ ìœ  ë ˆì´ë¸” ê°œìˆ˜ ê³„ì‚°
            logger.warning(f"meta.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. train.csv ê¸°ë°˜ í´ë˜ìŠ¤ ê°œìˆ˜: {num_classes}")

        # í´ë˜ìŠ¤ ê°œìˆ˜ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ ì˜¤ë¥˜ ì²˜ë¦¬
        if num_classes < 2:
            logger.error("ë¶„ë¥˜í•  í´ë˜ìŠ¤ê°€ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ë°ì´í„° ë˜ëŠ” ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            sys.exit(1)

        # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í•  (ì˜ˆ: 80% í•™ìŠµ, 20% ê²€ì¦)
        # stratify=full_df['target']: ë¶„í•  ì‹œ ê° í´ë˜ìŠ¤ì˜ ë¹„ìœ¨ì„ ì›ë³¸ ë°ì´í„°ì™€ ìœ ì‚¬í•˜ê²Œ ìœ ì§€
        train_df, val_df = train_test_split(
            full_df,
            test_size=0.2, # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (20%)
            random_state=model_config['SEED'], # ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
            stratify=full_df['target'] # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€ ì˜µì…˜
        )
        logger.info(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ: í•™ìŠµ {len(train_df)}ê°œ, ê²€ì¦ {len(val_df)}ê°œ")

        # ì´ë¯¸ì§€ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì • (IMAGES_DIR ì‚¬ìš©)
        if not IMAGES_DIR.is_dir():
             logger.error(f"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬({IMAGES_DIR})ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
             # í•„ìš”ì‹œ DATA_DIRì„ ëŒ€ì²´ ê²½ë¡œë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ì¢…ë£Œ
             # image_root_dir = DATA_DIR
             # logger.warning(f"IMAGES_DIR({IMAGES_DIR})ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ DATA_DIR({DATA_DIR})ë¥¼ ì´ë¯¸ì§€ ë£¨íŠ¸ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
             sys.exit(1)
        image_root_dir = IMAGES_DIR

        # ë°ì´í„° ë¡œë” ìƒì„±
        train_loader = build_dataloader(train_df, image_root_dir, model_config, is_train=True)
        val_loader = build_dataloader(val_df, image_root_dir, model_config, is_train=False)

    except Exception as e:
        logger.error(f"ë°ì´í„° ë¡œë”©/ë¶„í• /ë¡œë” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)

    # --- ëª¨ë¸ ìƒì„± ---
    try:
        # timm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
        model = timm.create_model(
            model_name=model_config['model_name'], # ì„¤ì • íŒŒì¼ì—ì„œ ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
            pretrained=True, # ImageNet ë“±ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
            num_classes=num_classes # ê³„ì‚°ëœ í´ë˜ìŠ¤ ê°œìˆ˜ ì ìš© (ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ ìˆ˜ì •)
        ).to(device) # ëª¨ë¸ì„ ì§€ì •ëœ ì¥ì¹˜(GPU/CPU)ë¡œ ì´ë™
        logger.info(f"ëª¨ë¸ '{model_config['model_name']}' ìƒì„± ì™„ë£Œ (í´ë˜ìŠ¤ ìˆ˜: {num_classes})")
    except Exception as e:
        logger.error(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ '{model_config['model_name']}': {e}", exc_info=True)
        sys.exit(1)

    # --- ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ---
    loss_fn = nn.CrossEntropyLoss() # ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— í‘œì¤€ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” CrossEntropyLoss
    optimizer = optim.Adam(model.parameters(), lr=model_config['LR']) # Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (ì„ íƒ ì‚¬í•­): íŠ¹ì • ì¡°ê±´ì— ë”°ë¼ í•™ìŠµë¥  ë™ì  ì¡°ì ˆ
    # ì˜ˆ: scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) # 5 ì—í­ë§ˆë‹¤ í•™ìŠµë¥  0.1ë°° ê°ì†Œ
    logger.info(f"ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss, ì˜µí‹°ë§ˆì´ì €: Adam (LR={model_config['LR']})")

    # --- í•™ìŠµ ë£¨í”„ ---
    epochs = model_config['EPOCHS'] # ì„¤ì • íŒŒì¼ì—ì„œ ì´ ì—í­ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    logger.info(f"--- ì´ {epochs} ì—í­ í•™ìŠµ ì‹œì‘ ---")

    best_val_f1 = 0.0 # ìµœê³  ê²€ì¦ F1 ì ìˆ˜ ì¶”ì ìš© ë³€ìˆ˜ ì´ˆê¸°í™”

    for epoch in range(epochs):
        # 1 ì—í­ í•™ìŠµ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥
        train_results = train_one_epoch(train_loader, model, optimizer, loss_fn, device, epoch, epochs)
        # 1 ì—í­ ê²€ì¦ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥
        val_results = validate_one_epoch(val_loader, model, loss_fn, device, epoch, epochs)

        # ì—í­ ê²°ê³¼ ë¡œê¹… (í•™ìŠµ/ê²€ì¦ ì†ì‹¤, ì •í™•ë„, F1 ìŠ¤ì½”ì–´)
        log_msg = (
            f"Epoch {epoch+1}/{epochs} - "
            f"Train Loss: {train_results['train_loss']:.4f}, Acc: {train_results['train_acc']:.4f}, F1: {train_results['train_f1']:.4f} | "
            f"Val Loss: {val_results['val_loss']:.4f}, Acc: {val_results['val_acc']:.4f}, F1: {val_results['val_f1']:.4f}"
        )
        logger.info(log_msg)

        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (ì‚¬ìš© ì‹œ)
        # scheduler.step()

        # --- ëª¨ë¸ ì €ì¥ (ì²´í¬í¬ì¸íŠ¸) ---
        current_val_f1 = val_results['val_f1']
        is_best = current_val_f1 > best_val_f1 # í˜„ì¬ F1ì´ ìµœê³  F1ë³´ë‹¤ ì¢‹ì€ì§€ í™•ì¸

        if is_best:
            best_val_f1 = current_val_f1
            # ì €ì¥ íŒŒì¼ëª… í˜•ì‹: best_model_epoch_{ì—í­ë²ˆí˜¸}_f1_{F1ì ìˆ˜}.pth
            save_path = OUTPUT_DIR / f"best_model_epoch_{epoch+1}_f1_{best_val_f1:.4f}.pth" # OUTPUT_DIR ì‚¬ìš©
            try:
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ëª¨ë¸ ìƒíƒœ, ì˜µí‹°ë§ˆì´ì € ìƒíƒœ, ì—í­ ë²ˆí˜¸, ìµœê³  ì ìˆ˜, ì„¤ì • ë“±)
                checkpoint = {
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(), # ëª¨ë¸ íŒŒë¼ë¯¸í„°
                    'optimizer_state_dict': optimizer.state_dict(), # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ
                    'best_val_f1': best_val_f1, # ìµœê³  ê²€ì¦ F1 ì ìˆ˜
                    # 'scheduler_state_dict': scheduler.state_dict(), # ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš© ì‹œ ìƒíƒœ ì €ì¥
                    'model_config': model_config # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ì„¤ì • ì €ì¥
                }
                torch.save(checkpoint, save_path)
                logger.info(f"ğŸš€ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {save_path} (Val F1: {best_val_f1:.4f})")
            except Exception as e:
                logger.error(f"ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)

        # ë§ˆì§€ë§‰ ì—í­ ëª¨ë¸ ì €ì¥ (ì„ íƒ ì‚¬í•­, state_dictë§Œ ì €ì¥)
        if epoch == epochs - 1:
             last_save_path = OUTPUT_DIR / f"last_model_epoch_{epoch+1}.pth" # OUTPUT_DIR ì‚¬ìš©
             try:
                 # ë§ˆì§€ë§‰ ì—í­ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë§Œ ì €ì¥
                 torch.save(model.state_dict(), last_save_path)
                 logger.info(f"ë§ˆì§€ë§‰ ì—í­ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {last_save_path}")
             except Exception as e:
                 logger.error(f"ë§ˆì§€ë§‰ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)

    logger.info(f"--- ì´ {epochs} ì—í­ í•™ìŠµ ì™„ë£Œ ---")
    logger.info(f"ìµœê³  ê²€ì¦ F1 ì ìˆ˜: {best_val_f1:.4f}")

# ===================== ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì§„ì…ì  =====================
if __name__ == "__main__":
    try:
        training() # ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ í˜¸ì¶œ
    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡œê¹… ë° ì¢…ë£Œ
        logger.exception(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}") # ì˜ˆì™¸ ì •ë³´ í¬í•¨ ë¡œê¹…
        sys.exit(1) # ë¹„ì •ìƒ ì¢…ë£Œ ìƒíƒœ ì½”ë“œ ë°˜í™˜
