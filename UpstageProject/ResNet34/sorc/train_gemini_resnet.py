# -*- coding: utf-8 -*-
"""
train_myresnet.py (ìˆ˜ì • ë²„ì „)

[ëª©ì ]
- .my_config.yaml ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸(ì˜ˆ: ResNet)ì„ ì§€ë„ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
- PyTorchì™€ timm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- í•™ìŠµëœ ëª¨ë¸ì€ í–¥í›„ íŠ¹ì§• ì¶”ì¶œ ë° ìœ ì‚¬ì„± ê²€ìƒ‰ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[ì£¼ìš” ë³€ê²½ ì‚¬í•­]
- ê²½ë¡œ ì„¤ì •, í•˜ì´í¼íŒŒë¼ë¯¸í„°, ë¡œê¹… ë©”ì‹œì§€ ë“±ì„ .my_config.yaml íŒŒì¼ì—ì„œ ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
- pathlibë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ë¡œë¥¼ ê°ì²´ ì§€í–¥ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
- ì„¤ì • íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬ ë¶€ë¶„ì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤.
- ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë° ë°ì´í„° ë¡œë”ê°€ ì„¤ì • íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
- í•™ìŠµ ë£¨í”„ì—ì„œ ì„¤ì • íŒŒì¼ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
- ëª¨ë¸ ì €ì¥ ì‹œ ì„¤ì • íŒŒì¼ì˜ ì¶œë ¥ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import os
import sys
import yaml
import random
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from PIL import Image, ExifTags, UnidentifiedImageError # UnidentifiedImageError ì¶”ê°€
import albumentations as A
from albumentations.pytorch import ToTensorV2
from tqdm import tqdm
import timm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split # ë°ì´í„° ë¶„í• ìš©

# --- ê²½ë¡œ ì„¤ì • ë° í™•ì¥ í•¨ìˆ˜ ---
def expand_path_vars(path_str: Union[str, Path], proj_dir: Path, data_dir: Optional[Path] = None, root_dir: Optional[Path] = None) -> Path:
    """YAMLì—ì„œ ì½ì€ ê²½ë¡œ ë¬¸ìì—´ ë‚´ì˜ ë³€ìˆ˜(${PROJ_DIR}, ${DATA_DIR}, $(ROOT_DIR))ì™€ ~, $~ ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."""
    if isinstance(path_str, Path): # ì´ë¯¸ Path ê°ì²´ë©´ ì ˆëŒ€ ê²½ë¡œ ë³´ì¥
        return path_str.resolve()
    if not isinstance(path_str, str):
        raise TypeError(f"Path must be a string or Path object, got: {type(path_str)}")

    expanded_str = path_str

    # $(ROOT_DIR) ì¹˜í™˜ (root_dirì´ ì œê³µëœ ê²½ìš°)
    if root_dir and '$(ROOT_DIR)' in expanded_str:
        expanded_str = expanded_str.replace('$(ROOT_DIR)', str(root_dir))

    # ${PROJ_DIR} ì¹˜í™˜
    expanded_str = expanded_str.replace('${PROJ_DIR}', str(proj_dir))

    # ${DATA_DIR} ì¹˜í™˜ (data_dirì´ ê²°ì •ëœ í›„ì— ì‚¬ìš© ê°€ëŠ¥)
    # YAMLì˜ ${DATAS_DIR}ëŠ” ${DATA_DIR}ì˜ ì˜¤íƒ€ë¡œ ê°„ì£¼
    if data_dir:
        expanded_str = expanded_str.replace('${DATA_DIR}', str(data_dir))
        expanded_str = expanded_str.replace('${DATAS_DIR}', str(data_dir)) # ì˜¤íƒ€ ê°€ëŠ¥ì„± ê³ ë ¤

    # ~ (í™ˆ ë””ë ‰í† ë¦¬) í™•ì¥ ë° ë¹„í‘œì¤€ $~/ ì²˜ë¦¬ ì‹œë„
    if '$~/' in expanded_str:
        expanded_str = expanded_str.replace('$~/','~/')
    if expanded_str.startswith('~'):
        expanded_str = os.path.expanduser(expanded_str)

    # Path ê°ì²´ ìƒì„± ë° ì ˆëŒ€ ê²½ë¡œí™” (resolve)
    try:
        final_path = Path(expanded_str)
        # ìƒëŒ€ ê²½ë¡œì¼ ê²½ìš° PROJ_DIR ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œí™” ì‹œë„
        if not final_path.is_absolute():
             final_path = (proj_dir / final_path).resolve()
        else:
             final_path = final_path.resolve() # ì´ë¯¸ ì ˆëŒ€ê²½ë¡œì—¬ë„ resolve()ëŠ” ì•ˆì „
        return final_path
    except Exception as e:
        print(f"ì˜¤ë¥˜: ê²½ë¡œ ë¬¸ìì—´ '{path_str}' (í™•ì¥ í›„: '{expanded_str}') ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        raise

# --- ì„¤ì • ë¡œë“œ ë° ê²½ë¡œ ë³€ìˆ˜ ì„¤ì • ---
try:
    current_file_path = Path(__file__).resolve()
    WORK_DIR_SCRIPT = current_file_path.parent
    PROJ_DIR_SCRIPT = WORK_DIR_SCRIPT.parent
    dir_config_path_yaml = PROJ_DIR_SCRIPT / ".my_config.yaml"

    if not dir_config_path_yaml.is_file():
        print(f"âŒ í”„ë¡œì íŠ¸ ê²½ë¡œêµ¬ì„± ì„¤ì • íŒŒì¼({dir_config_path_yaml})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit("í”„ë¡œì íŠ¸ ê²½ë¡œêµ¬ì„± ì„¤ì • íŒŒì¼ ì—†ìŒ")

    with open(dir_config_path_yaml, "r", encoding="utf-8") as file:
        dir_config = yaml.safe_load(file)

    # --- ê¸°ë³¸ ê²½ë¡œ ê²°ì • ---
    # ROOT_DIR (ì„¤ì • íŒŒì¼ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì¶”ì •)
    raw_root_dir = dir_config.get('ROOT_DIR')
    ROOT_DIR = Path(raw_root_dir).resolve() if raw_root_dir else PROJ_DIR_SCRIPT.parent # ê¸°ë³¸ê°’: í”„ë¡œì íŠ¸ ìƒìœ„

    # PROJ_DIR (ì„¤ì • íŒŒì¼ ìš°ì„ , ì—†ìœ¼ë©´ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ë°˜)
    raw_proj_dir = dir_config.get('resnet34_path', {}).get('PROJ_DIR')
    if raw_proj_dir:
        PROJ_DIR = expand_path_vars(raw_proj_dir, PROJ_DIR_SCRIPT, root_dir=ROOT_DIR) # ROOT_DIR ì°¸ì¡° í™•ì¥
    else:
        PROJ_DIR = PROJ_DIR_SCRIPT
        print(f"ì •ë³´: ì„¤ì • íŒŒì¼ì— PROJ_DIRì´ ì—†ì–´ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ë°˜ ê²½ë¡œ ì‚¬ìš©: {PROJ_DIR}")

    # --- ì„¸ë¶€ ê²½ë¡œ ì„¤ì • (ì„¤ì • íŒŒì¼ ê°’ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ + í™•ì¥) ---
    resnet34_paths = dir_config.get('resnet34_path', {})
    worker_paths = resnet34_paths.get('worker_path', {})
    dataset_paths = resnet34_paths.get('dataset_path', {})
    output_paths = resnet34_paths.get('output_path', {})
    message_str = resnet34_paths.get('MESSAGE', {})
    model_params_cfg = resnet34_paths.get('model_params', {}) # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¹ì…˜ ë¡œë“œ

    # WORK_DIR
    raw_work_dir = worker_paths.get('WORK_DIR', '${PROJ_DIR}/sorc') # ê¸°ë³¸ê°’ì— ë³€ìˆ˜ ì‚¬ìš©
    WORK_DIR = expand_path_vars(raw_work_dir, PROJ_DIR, root_dir=ROOT_DIR)

    # LOG_DIR
    raw_log_dir = worker_paths.get('LOG_DIR', '${PROJ_DIR}/logs')
    LOG_DIR = expand_path_vars(raw_log_dir, PROJ_DIR, root_dir=ROOT_DIR)

    # DATA_DIR
    raw_data_dir = dataset_paths.get('DATA_DIR', '${PROJ_DIR}/data')
    DATA_DIR = expand_path_vars(raw_data_dir, PROJ_DIR, root_dir=ROOT_DIR)

    # IMAGES_DIR (DATA_DIR ê²°ì • í›„ í™•ì¥)
    raw_images_dir = dataset_paths.get('IMAGES_DIR', '${DATA_DIR}/train') # ê¸°ë³¸ê°’ ë³€ê²½: í•™ìŠµ ì´ë¯¸ì§€ ê²½ë¡œ
    IMAGES_DIR = expand_path_vars(raw_images_dir, PROJ_DIR, data_dir=DATA_DIR, root_dir=ROOT_DIR)

    # TRAIN_CSV ê²½ë¡œ
    raw_train_csv = dataset_paths.get('TRAIN_CSV', '${DATA_DIR}/train.csv')
    TRAIN_CSV_PATH = expand_path_vars(raw_train_csv, PROJ_DIR, data_dir=DATA_DIR, root_dir=ROOT_DIR)

    # META_CSV ê²½ë¡œ
    raw_meta_csv = dataset_paths.get('META_CSV', '${DATA_DIR}/meta.csv')
    META_CSV_PATH = expand_path_vars(raw_meta_csv, PROJ_DIR, data_dir=DATA_DIR, root_dir=ROOT_DIR)

    # OUTPUT_DIR
    raw_output_dir = output_paths.get('OUTPUT_DIR', '${PROJ_DIR}/outputs')
    OUTPUT_DIR = expand_path_vars(raw_output_dir, PROJ_DIR, root_dir=ROOT_DIR)

    # ë””ë ‰í† ë¦¬ ìƒì„±
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)

    # --- ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ ---
    model_config = {
        'LR': float(model_params_cfg.get('LR', 1e-3)),
        'EPOCHS': int(model_params_cfg.get('EPOCHS', 10)),
        'BATCH_SIZE': int(model_params_cfg.get('BATCH_SIZE', 32)),
        'NUM_WORKERS': int(model_params_cfg.get('NUM_WORKERS', 2)),
        'IMG_SIZE': int(model_params_cfg.get('IMG_SIZE', 224)),
        'MODEL_NAME': model_params_cfg.get('MODEL_NAME', 'resnet34'),
        'SEED': int(model_params_cfg.get('SEED', 42))
    }

except FileNotFoundError as e:
    print(f"âŒ ì„¤ì • íŒŒì¼ ê´€ë ¨ ì˜¤ë¥˜: {e}")
    sys.exit(1)
except KeyError as e:
    print(f"âŒ ì„¤ì • íŒŒì¼ í‚¤ ì˜¤ë¥˜: í•„ìš”í•œ í‚¤ '{e}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)
except Exception as e:
    print(f"âŒ ì´ˆê¸° ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# --- ë¡œê±° ì„¤ì • ---
worker_name = Path(__file__).stem
log_file_path = LOG_DIR / f"{worker_name}.log"
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)-6s - %(filename)s:%(lineno)d - %(message)s',
                    handlers=[logging.StreamHandler(sys.stdout),
                              logging.FileHandler(log_file_path, encoding='utf-8')])
logger = logging.getLogger(__name__)

# --- ë¡œê¹… ë©”ì‹œì§€ (ì„¤ì • íŒŒì¼ ë©”ì‹œì§€ ì‚¬ìš©) ---
logger.info(f"{message_str.get('START', 'ë°œììš± ë§Œë“¤ê¸° ì‹œì‘')}:10s: {worker_name}")
logger.info(f"{message_str.get('PROJ_DIR', 'í”„ë¡œì íŠ¸ ìœ„ì¹˜')}:10s: {PROJ_DIR}")
logger.info(f"{message_str.get('WORK_DIR', 'ì‘ì—… ë””ë ‰í† ë¦¬')}:10s: {WORK_DIR}")
logger.info(f"{message_str.get('DATA_DIR', 'ë°ì´í„° ìœ„ì¹˜')}:10s: {DATA_DIR}")
logger.info(f"{message_str.get('IMAGES_DIR', 'ì´ë¯¸ì§€ ìœ„ì¹˜')}:10s: {IMAGES_DIR}")
logger.info(f"{message_str.get('OUTPUT_DIR', 'ì¶œë ¥ ìœ„ì¹˜')}:10s: {OUTPUT_DIR}")
logger.info(f"{message_str.get('LOG_DIR', 'ë¡œê·¸ ìœ„ì¹˜')}:10s: {LOG_DIR}")
logger.info(f"í•™ìŠµ ë°ì´í„° CSV: {TRAIN_CSV_PATH}")
logger.info(f"ë©”íƒ€ ë°ì´í„° CSV: {META_CSV_PATH}")
logger.info(f"ëª¨ë¸ ì„¤ì •: {model_config}")


# --- í™˜ê²½ ì´ˆê¸°í™” ---
def init_env(seed: int):
    """í™˜ê²½ ì‹œë“œ ê³ ì • í•¨ìˆ˜"""
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if using multi-GPU
    torch.backends.cudnn.benchmark = True
    logger.info(f"í™˜ê²½ ì‹œë“œ ê³ ì • ì™„ë£Œ (SEED={seed})")

# --- ì´ë¯¸ì§€ íšŒì „ ì²˜ë¦¬ ---
EXIF_ORIENTATION_TAG = None
for k, v in ExifTags.TAGS.items():
    if v == 'Orientation':
        EXIF_ORIENTATION_TAG = k
        break

def rotate_image_based_on_exif(image: Image.Image) -> Image.Image:
    """EXIF ì •ë³´ì— ë”°ë¼ ì´ë¯¸ì§€ë¥¼ íšŒì „ì‹œí‚µë‹ˆë‹¤."""
    if EXIF_ORIENTATION_TAG is None: return image
    try:
        exif = image.getexif()
        orientation = exif.get(EXIF_ORIENTATION_TAG)
    except Exception:
        orientation = None

    if orientation == 2: return image.transpose(Image.FLIP_LEFT_RIGHT)
    elif orientation == 3: return image.rotate(180)
    elif orientation == 4: return image.rotate(180).transpose(Image.FLIP_LEFT_RIGHT)
    elif orientation == 5: return image.rotate(-90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)
    elif orientation == 6: return image.rotate(-90, expand=True)
    elif orientation == 7: return image.rotate(90, expand=True).transpose(Image.FLIP_LEFT_RIGHT)
    elif orientation == 8: return image.rotate(90, expand=True)
    return image

# --- ì´ë¯¸ì§€ ë¶„ë¥˜ ë°ì´í„°ì…‹ ---
class ClassificationDataset(Dataset):
    """
    train.csv íŒŒì¼ì„ ì½ì–´ ì´ë¯¸ì§€ ë¶„ë¥˜ í•™ìŠµìš© ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” Dataset í´ë˜ìŠ¤.
    """
    def __init__(self, df: pd.DataFrame, image_root: Path, img_size: int, transform: Optional[A.Compose] = None):
        """
        Args:
            df (pd.DataFrame): 'ID' (ì´ë¯¸ì§€ íŒŒì¼ëª…)ì™€ 'target' (ë ˆì´ë¸”) ì»¬ëŸ¼ì„ í¬í•¨í•˜ëŠ” ë°ì´í„°í”„ë ˆì„.
            image_root (Path): ì´ë¯¸ì§€ íŒŒì¼ë“¤ì´ ìˆëŠ” ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
            img_size (int): ëª¨ë¸ ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ë”ë¯¸ ë°ì´í„° ìƒì„± ì‹œ ì‚¬ìš©).
            transform (Optional[A.Compose]): Albumentations ë³€í™˜ íŒŒì´í”„ë¼ì¸.
        """
        self.df = df.reset_index(drop=True)
        self.image_root = image_root
        self.transform = transform
        self.img_size = img_size # ë”ë¯¸ ë°ì´í„° ìƒì„± ìœ„í•´ ì €ì¥
        logger.info(f"Dataset ìƒì„±: {len(self.df)}ê°œ ìƒ˜í”Œ, ì´ë¯¸ì§€ ë£¨íŠ¸: {self.image_root}")

    def __len__(self) -> int:
        return len(self.df)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        image_id = self.df.loc[idx, 'ID']
        target = int(self.df.loc[idx, 'target'])
        image_path = self.image_root / image_id

        try:
            image = Image.open(image_path).convert('RGB')
            image = rotate_image_based_on_exif(image)
            image_np = np.array(image)

        except FileNotFoundError:
            logger.warning(f"ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_path} (ì¸ë±ìŠ¤ {idx}). ë”ë¯¸ ë°ì´í„° ë°˜í™˜.")
            dummy_image = torch.zeros((3, self.img_size, self.img_size), dtype=torch.float32)
            return dummy_image, -1
        except UnidentifiedImageError: # PILì´ ì‹ë³„í•  ìˆ˜ ì—†ëŠ” ì´ë¯¸ì§€ ì²˜ë¦¬
             logger.warning(f"ì´ë¯¸ì§€ íŒŒì¼ ì‹ë³„ ë¶ˆê°€: {image_path}. ë”ë¯¸ ë°ì´í„° ë°˜í™˜.")
             dummy_image = torch.zeros((3, self.img_size, self.img_size), dtype=torch.float32)
             return dummy_image, -1
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ë¡œë”©/ì²˜ë¦¬ ì˜¤ë¥˜ {image_path}: {e}. ë”ë¯¸ ë°ì´í„° ë°˜í™˜.", exc_info=False) # exc_info=Falseë¡œ ë³€ê²½ (ë„ˆë¬´ ê¸¸ì–´ì§ ë°©ì§€)
            dummy_image = torch.zeros((3, self.img_size, self.img_size), dtype=torch.float32)
            return dummy_image, -1

        if self.transform:
            try:
                transformed = self.transform(image=image_np)
                image_tensor = transformed['image']
            except Exception as e:
                 logger.error(f"Albumentations ë³€í™˜ ì ìš© ì˜¤ë¥˜ {image_path}: {e}. ë”ë¯¸ ë°ì´í„° ë°˜í™˜.", exc_info=False)
                 image_tensor = torch.zeros((3, self.img_size, self.img_size), dtype=torch.float32)
                 return image_tensor, -1
        else:
            # ê¸°ë³¸ ë³€í™˜ (Albumentations ì—†ì„ ê²½ìš°)
            image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).float() / 255.0

        return image_tensor, target

# --- ë°ì´í„° ë¡œë” ìƒì„± ---
def build_dataloader(df: pd.DataFrame, image_root: Path, model_cfg: Dict, is_train: bool) -> DataLoader:
    """DataLoader ìƒì„± í•¨ìˆ˜"""
    img_size = model_cfg['IMG_SIZE'] # í‚¤ ì´ë¦„ ë³€ê²½ IMG_SIZE
    batch_size = model_cfg['BATCH_SIZE']
    num_workers = model_cfg['NUM_WORKERS']

    if is_train:
        transform = A.Compose([
            A.Resize(height=img_size, width=img_size),
            A.HorizontalFlip(p=0.5),
            A.Rotate(limit=15, p=0.3),
            A.RandomBrightnessContrast(p=0.2),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ])
        logger.info("í•™ìŠµìš© ë°ì´í„° ë³€í™˜ ì„¤ì • ì™„ë£Œ (ì¦ê°• í¬í•¨).")
    else:
        transform = A.Compose([
            A.Resize(height=img_size, width=img_size),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ])
        logger.info("ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë³€í™˜ ì„¤ì • ì™„ë£Œ (ì¦ê°• ì—†ìŒ).")

    # ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ img_size ì „ë‹¬
    dataset = ClassificationDataset(df=df, image_root=image_root, img_size=img_size, transform=transform)

    loader = DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        shuffle=is_train,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    )
    logger.info(f"{'í•™ìŠµ' if is_train else 'ê²€ì¦'} DataLoader ìƒì„± ì™„ë£Œ (Batch size: {batch_size}, Num workers: {num_workers})")
    return loader

# --- í•™ìŠµ ë° ê²€ì¦ ë£¨í”„ ---
def train_one_epoch(loader: DataLoader, model: nn.Module, optimizer: optim.Optimizer, loss_fn: nn.Module, device: torch.device, epoch_num: int, total_epochs: int) -> Dict[str, float]:
    """1 ì—í­ í•™ìŠµ"""
    model.train()
    total_loss = 0.0
    all_preds = []
    all_targets = []
    dummy_data_count = 0

    pbar = tqdm(loader, desc=f"Epoch {epoch_num+1}/{total_epochs} Training")
    for images, targets in pbar:
        # ë”ë¯¸ ë°ì´í„°(-1 ë ˆì´ë¸”) ê±´ë„ˆë›°ê¸°
        valid_indices = targets != -1
        if not valid_indices.all(): # ë”ë¯¸ ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´
            num_dummy = (~valid_indices).sum().item()
            dummy_data_count += num_dummy
            # ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§
            images = images[valid_indices]
            targets = targets[valid_indices]
            if images.size(0) == 0: # ë°°ì¹˜ ì „ì²´ê°€ ë”ë¯¸ ë°ì´í„°ë©´ ê±´ë„ˆë›°ê¸°
                logger.warning(f"ë°°ì¹˜ ì „ì²´ê°€ ë”ë¯¸ ë°ì´í„° ({num_dummy}ê°œ). ê±´ë„ˆ<0xEB><0><0x8F>ë‹ˆë‹¤.")
                continue
            logger.warning(f"ë°°ì¹˜ ë‚´ ë”ë¯¸ ë°ì´í„° {num_dummy}ê°œ ì œì™¸í•˜ê³  ì²˜ë¦¬.")

        images = images.to(device, non_blocking=True)
        targets = targets.to(device, non_blocking=True)

        optimizer.zero_grad()
        preds = model(images)
        loss = loss_fn(preds, targets)

        if torch.isnan(loss):
            logger.error(f"NaN ì†ì‹¤ ê°ì§€ (Epoch {epoch_num+1}). ë°°ì¹˜ë¥¼ ê±´ë„ˆ<0xEB><0><0x8F>ë‹ˆë‹¤.")
            continue

        loss.backward()
        optimizer.step()

        total_loss += loss.item() * images.size(0)
        all_preds.extend(preds.argmax(dim=1).detach().cpu().numpy())
        all_targets.extend(targets.detach().cpu().numpy())
        pbar.set_postfix(loss=f"{loss.item():.4f}")

    # ìœ íš¨í•˜ê²Œ ì²˜ë¦¬ëœ ìƒ˜í”Œ ìˆ˜ ê³„ì‚° (ì „ì²´ - ë”ë¯¸)
    num_valid_samples = len(all_targets) # í•„í„°ë§ í›„ ì‹¤ì œ ì²˜ë¦¬ëœ ìƒ˜í”Œ ìˆ˜
    avg_loss = total_loss / num_valid_samples if num_valid_samples > 0 else 0.0
    accuracy = accuracy_score(all_targets, all_preds) if num_valid_samples > 0 else 0.0
    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0) if num_valid_samples > 0 else 0.0

    if dummy_data_count > 0:
        logger.warning(f"Epoch {epoch_num+1} Training: ì´ {dummy_data_count}ê°œì˜ ë”ë¯¸ ë°ì´í„° ì²˜ë¦¬ë¨.")

    return {"train_loss": avg_loss, "train_acc": accuracy, "train_f1": f1}

def validate_one_epoch(loader: DataLoader, model: nn.Module, loss_fn: nn.Module, device: torch.device, epoch_num: int, total_epochs: int) -> Dict[str, float]:
    """1 ì—í­ ê²€ì¦"""
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_targets = []
    dummy_data_count = 0

    pbar = tqdm(loader, desc=f"Epoch {epoch_num+1}/{total_epochs} Validation")
    with torch.no_grad():
        for images, targets in pbar:
            # ë”ë¯¸ ë°ì´í„° ì²˜ë¦¬
            valid_indices = targets != -1
            if not valid_indices.all():
                num_dummy = (~valid_indices).sum().item()
                dummy_data_count += num_dummy
                images = images[valid_indices]
                targets = targets[valid_indices]
                if images.size(0) == 0:
                    continue

            images = images.to(device, non_blocking=True)
            targets = targets.to(device, non_blocking=True)

            preds = model(images)
            loss = loss_fn(preds, targets)

            if torch.isnan(loss):
                logger.error(f"NaN ì†ì‹¤ ê°ì§€ (Epoch {epoch_num+1} Validation). ë°°ì¹˜ë¥¼ ê±´ë„ˆ<0xEB><0><0x8F>ë‹ˆë‹¤.")
                continue

            total_loss += loss.item() * images.size(0)
            all_preds.extend(preds.argmax(dim=1).cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            pbar.set_postfix(loss=f"{loss.item():.4f}")

    num_valid_samples = len(all_targets)
    avg_loss = total_loss / num_valid_samples if num_valid_samples > 0 else 0.0
    accuracy = accuracy_score(all_targets, all_preds) if num_valid_samples > 0 else 0.0
    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0) if num_valid_samples > 0 else 0.0

    if dummy_data_count > 0:
        logger.warning(f"Epoch {epoch_num+1} Validation: ì´ {dummy_data_count}ê°œì˜ ë”ë¯¸ ë°ì´í„° ì²˜ë¦¬ë¨.")

    return {"val_loss": avg_loss, "val_acc": accuracy, "val_f1": f1}


# --- ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ ---
def training():
    """ë©”ì¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    init_env(seed=model_config['SEED'])
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # --- ë°ì´í„° ë¡œë“œ ë° ë¶„í•  ---
    try:
        if not TRAIN_CSV_PATH.is_file():
            logger.error(f"í•™ìŠµ ë°ì´í„° íŒŒì¼({TRAIN_CSV_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        full_df = pd.read_csv(TRAIN_CSV_PATH)
        logger.info(f"{TRAIN_CSV_PATH} ë¡œë“œ ì™„ë£Œ. ì´ {len(full_df)}ê°œ ë°ì´í„°.")

        # í´ë˜ìŠ¤ ê°œìˆ˜ í™•ì¸
        if META_CSV_PATH.is_file():
            meta_df = pd.read_csv(META_CSV_PATH)
            num_classes = len(meta_df)
            logger.info(f"{META_CSV_PATH} ê¸°ë°˜ í´ë˜ìŠ¤ ê°œìˆ˜: {num_classes}")
        else:
            num_classes = full_df['target'].nunique()
            logger.warning(f"{META_CSV_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {TRAIN_CSV_PATH} ê¸°ë°˜ í´ë˜ìŠ¤ ê°œìˆ˜: {num_classes}")

        if num_classes < 2:
            logger.error("ë¶„ë¥˜í•  í´ë˜ìŠ¤ê°€ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ë°ì´í„° ë˜ëŠ” ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            sys.exit(1)

        # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
        train_df, val_df = train_test_split(
            full_df,
            test_size=0.2,
            random_state=model_config['SEED'],
            stratify=full_df['target']
        )
        logger.info(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ: í•™ìŠµ {len(train_df)}ê°œ, ê²€ì¦ {len(val_df)}ê°œ")

        # ì´ë¯¸ì§€ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ í™•ì¸
        if not IMAGES_DIR.is_dir():
             logger.error(f"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬({IMAGES_DIR})ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
             sys.exit(1)
        image_root_dir = IMAGES_DIR

        # ë°ì´í„° ë¡œë” ìƒì„±
        train_loader = build_dataloader(train_df, image_root_dir, model_config, is_train=True)
        val_loader = build_dataloader(val_df, image_root_dir, model_config, is_train=False)

    except Exception as e:
        logger.error(f"ë°ì´í„° ë¡œë”©/ë¶„í• /ë¡œë” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)

    # --- ëª¨ë¸ ìƒì„± ---
    try:
        model = timm.create_model(
            model_name=model_config['MODEL_NAME'],
            pretrained=True,
            num_classes=num_classes
        ).to(device)
        logger.info(f"ëª¨ë¸ '{model_config['MODEL_NAME']}' ìƒì„± ì™„ë£Œ (í´ë˜ìŠ¤ ìˆ˜: {num_classes})")
    except Exception as e:
        logger.error(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ '{model_config['MODEL_NAME']}': {e}", exc_info=True)
        sys.exit(1)

    # --- ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ---
    loss_fn = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=model_config['LR'])
    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) # í•„ìš”ì‹œ ì‚¬ìš©
    logger.info(f"ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss, ì˜µí‹°ë§ˆì´ì €: Adam (LR={model_config['LR']})")

    # --- í•™ìŠµ ë£¨í”„ ---
    epochs = model_config['EPOCHS']
    logger.info(f"--- ì´ {epochs} ì—í­ í•™ìŠµ ì‹œì‘ ---")
    best_val_f1 = 0.0

    for epoch in range(epochs):
        train_results = train_one_epoch(train_loader, model, optimizer, loss_fn, device, epoch, epochs)
        val_results = validate_one_epoch(val_loader, model, loss_fn, device, epoch, epochs)

        log_msg = (
            f"Epoch {epoch+1}/{epochs} - "
            f"Train Loss: {train_results['train_loss']:.4f}, Acc: {train_results['train_acc']:.4f}, F1: {train_results['train_f1']:.4f} | "
            f"Val Loss: {val_results['val_loss']:.4f}, Acc: {val_results['val_acc']:.4f}, F1: {val_results['val_f1']:.4f}"
        )
        logger.info(log_msg)

        # scheduler.step() # í•„ìš”ì‹œ ì‚¬ìš©

        # --- ëª¨ë¸ ì €ì¥ ---
        current_val_f1 = val_results['val_f1']
        is_best = current_val_f1 > best_val_f1

        # í•­ìƒ ë§ˆì§€ë§‰ ì—í­ ëª¨ë¸ ì €ì¥
        last_save_path = OUTPUT_DIR / f"last_model_epoch_{epoch+1}.pth"
        try:
            torch.save(model.state_dict(), last_save_path)
            # logger.info(f"ë§ˆì§€ë§‰ ì—í­ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {last_save_path}") # ë„ˆë¬´ ìì£¼ ë¡œê¹…ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
        except Exception as e:
            logger.error(f"ë§ˆì§€ë§‰ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=False)

        if is_best:
            best_val_f1 = current_val_f1
            save_path = OUTPUT_DIR / f"best_model_f1_{best_val_f1:.4f}.pth" # íŒŒì¼ëª… ê°„ì†Œí™”
            try:
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (state_dictë§Œ ì €ì¥í•˜ê±°ë‚˜ ì „ì²´ checkpoint ì €ì¥)
                # ì—¬ê¸°ì„œëŠ” state_dictë§Œ ì €ì¥
                torch.save(model.state_dict(), save_path)
                logger.info(f"ğŸš€ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path} (Val F1: {best_val_f1:.4f})")

                # ì´ì „ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‚­ì œ (ì„ íƒ ì‚¬í•­)
                # for f in OUTPUT_DIR.glob("best_model_f1_*.pth"):
                #     if f != save_path:
                #         f.unlink()

            except Exception as e:
                logger.error(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)


    logger.info(f"--- ì´ {epochs} ì—í­ í•™ìŠµ ì™„ë£Œ ---")
    logger.info(f"ìµœê³  ê²€ì¦ F1 ì ìˆ˜: {best_val_f1:.4f}")

# ===================== ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì§„ì…ì  =====================
if __name__ == "__main__":
    try:
        training()
    except Exception as e:
        logger.exception(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)
    logger.info(f"{message_str.get('ENDED', 'ë°œììš± ë§Œë“¤ê¸° ë§ˆì¹¨')}: {worker_name}")
